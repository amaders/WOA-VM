{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ce29669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/WOA-VM/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from nnsight import NNsight, LanguageModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b8151ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory cache\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "\n",
    "# Call this before loading the model\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ddc79f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_prompts_to_equal_tokens(tokenizer, a: str, b: str, add_special_tokens: bool = True) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Right-pad the shorter of (a, b) with a whitespace-like unit so that\n",
    "    their token counts match under `tokenizer`.\n",
    "    \"\"\"\n",
    "    pad_unit = \" \"\n",
    "\n",
    "    def toklen(s: str) -> int:\n",
    "        return len(tokenizer(s, add_special_tokens=add_special_tokens).input_ids)\n",
    "\n",
    "    len_a = toklen(a)\n",
    "    len_b = toklen(b)\n",
    "    target = max(len_a, len_b)\n",
    "\n",
    "    # Right-pad the shorter prompt until its token length == target\n",
    "    def right_pad_to(s: str, target_len: int) -> str:\n",
    "        while toklen(s) < target_len:\n",
    "            s += pad_unit\n",
    "        return s\n",
    "\n",
    "    if len_a < target:\n",
    "        a = right_pad_to(a, target)\n",
    "    if len_b < target:\n",
    "        b = right_pad_to(b, target)\n",
    "    # If one overshot (rare with single-token pad), bump the other to match\n",
    "    la, lb = toklen(a), toklen(b)\n",
    "    if la != lb:\n",
    "        target = max(la, lb)\n",
    "        a = right_pad_to(a, target)\n",
    "        b = right_pad_to(b, target)\n",
    "    \n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6519056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(152064, 5120)\n",
      "    (layers): ModuleList(\n",
      "      (0-47): 48 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "          (k_proj): Linear(in_features=5120, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=5120, out_features=1024, bias=True)\n",
      "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
      "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
      "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((5120,), eps=1e-05)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",
      "  (generator): Generator(\n",
      "    (streamer): Streamer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen2.5-14B\"  # or the base model\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-14B\",\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,  \n",
    ")\n",
    "\n",
    "llm = LanguageModel(model_id, device_map=\"auto\")\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3121675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with llm.trace(\"Love\"):\n",
    "#     # user-defined code to access internal model components\n",
    "#     hs_all_love = llm.model.layers[1].input[0].save()\n",
    "#     output = llm.output.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2392339",
   "metadata": {},
   "source": [
    "STEERING VECTOR IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "96b0971c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "really ugly hatred anger pain suffering hate hate hate frown lovely pretty beautiful lovely cheerful smile                                                                                                                                                                                                                                                                                                                                                  \n"
     ]
    }
   ],
   "source": [
    "encouraged_sentiment = \"really ugly hatred anger pain suffering hate hate hate frown\" \n",
    "discouraged_sentiment = \"lovely pretty beautiful lovely cheerful smile\"\n",
    "encouraged_sentiment, discouraged_sentiment = pad_prompts_to_equal_tokens(tok, encouraged_sentiment, discouraged_sentiment)\n",
    "print(encouraged_sentiment, discouraged_sentiment)\n",
    "len_steering_vector = len(tok(encouraged_sentiment).input_ids)\n",
    "residual_stream_layer = 3 # i.e. residual stream inputs at layer 0,1,2.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f3278227",
   "metadata": {},
   "outputs": [],
   "source": [
    "with llm.trace(encouraged_sentiment):\n",
    "    # user-defined code to access internal model components\n",
    "    encouraged_sentiment_stream = llm.model.layers[residual_stream_layer].input[0].save()\n",
    "with llm.trace(discouraged_sentiment):\n",
    "    # user-defined code to access internal model components\n",
    "    discouraged_sentiment_stream = llm.model.layers[residual_stream_layer].input[0].save()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8c8cca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "steering_vector = encouraged_sentiment_stream - discouraged_sentiment_stream\n",
    "\n",
    "\n",
    "test_query = \"Today I feel strongly that you are a person that makes me feel\"\n",
    "len_test_query = len(tok(test_query).input_ids)\n",
    "tensor_addition_cutoff = min(len_test_query,len_steering_vector)\n",
    "\n",
    "max_tokens = 30\n",
    "\n",
    "c = 3 #steering strength\n",
    "a = 1 #sequence fixing\n",
    "topk = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "41062b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Most Likely Words:\n",
      "1:  like\n",
      "2:  so\n",
      "3:  better\n",
      "4:  angry\n",
      "5:  that\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "with llm.trace(test_query):\n",
    "    llm.model.layers[residual_stream_layer].input[0][:tensor_addition_cutoff] += c*steering_vector[:tensor_addition_cutoff]\n",
    "    output = llm.output.save()\n",
    "\n",
    "# after exiting the tracing context, we can access any values that were saved\n",
    "\n",
    "output_logits = output[\"logits\"]\n",
    "\n",
    "# Get the logits for the last token position\n",
    "last_token_logits = output_logits[0, -1]\n",
    "\n",
    "# Get the top 5 most likely tokens and their probabilities\n",
    "top_probs, top_indices = last_token_logits.topk(topk, dim=-1)\n",
    "top_words = [llm.tokenizer.decode([idx.item()]) for idx in top_indices]\n",
    "\n",
    "print(\"Top 5 Most Likely Words:\")\n",
    "for i in range(topk):\n",
    "    print(f\"{i+1}: {top_words[i]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c9788db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(tok(test_query, add_special_tokens=True).input_ids) < max_tokens:\n",
    "\n",
    "    with torch.no_grad():                     # no autograd graph\n",
    "        with llm.trace(test_query):\n",
    "            R = llm.model.layers[residual_stream_layer].input[0]   # [L_q, d_model]\n",
    "\n",
    "            # write via DETACHED view to avoid leaf in-place grad error\n",
    "            Rv = R.detach()\n",
    "\n",
    "            \n",
    "            Rv[:tensor_addition_cutoff].add_((c * steering_vector[:tensor_addition_cutoff]).to(R.device, R.dtype))\n",
    "\n",
    "            # save ONLY logits; don't save whole output object\n",
    "            logits = llm.output.logits.save()\n",
    "\n",
    "    # take last-step logits, move to CPU, pick next token\n",
    "    last = logits[-1] if logits.dim() == 2 else logits[0, -1]\n",
    "    next_id = int(last.argmax(dim=-1).item())\n",
    "\n",
    "    # free GPU asap\n",
    "    del logits\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.ipc_collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # append next token\n",
    "    next_txt = llm.tokenizer.decode([next_id], skip_special_tokens=False)\n",
    "    test_query += next_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ba6294b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Today I feel strongly that you are a person that makes me feel like I'm a failure. I'm not sure if I'm doing enough to help\""
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f648893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4de37d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
