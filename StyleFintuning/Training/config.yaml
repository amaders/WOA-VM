wandb_project: llama3-subjective-sft #
wandb_name: llama3-8b_subjective_sft_reasoning
wandb_mode: online
wandb_log_model: "end"                        # "checkpoint" logs every save; "end" only final model
logging_steps: 10

base_model: meta-llama/Meta-Llama-3-8B-Instruct        # Hugging Face model ID for LLaMA3-8B-Instruct
model_type: LlamaForCausalLM                          # Model architecture class (Llama causal language model)
tokenizer_type: AutoTokenizer                        # Tokenizer class matching the LLaMA model
is_llama_derived_model: true                          # Indicates this model is LLaMA-based (for internal optimizations)

output_dir: ./output/llama3-8b-sft                    # Directory to save the fine-tuned model
num_epochs: 1                                         # Train for 1 epoch over the dataset

# to fit memory at the larger window on a single H100:
gradient_checkpointing: true
deepspeed: StyleFintuning/Training/zero2_offload.json
micro_batch_size: 4                # was 16
gradient_accumulation_steps: 32    # keep effective batch = 8 * 32 = 256
bf16: true

sequence_len: 2048
max_prompt_len: 2048


learning_rate: 0.00002                                # Initial learning rate for the optimizer (2e-5)
lr_scheduler: cosine                                  # Use a cosine decay learning rate scheduler
optimizer: adamw_torch                             

datasets:

  - path: StyleFintuning/Data/direct_response/train.jsonl #path to trian dataset
    type: chat_template 

    #chat_template: chatml #we are using ChatML instruction format for training
    chat_template: llama3 #we are using llama3 instruction format for training llama models

    message_property_mappings:  # our mapping is consistent with ChatML
      role: role
      content: content
    
    #define how to map role keys to roles (this is may be redundant, but whatever)
    roles:
      assistant:
        - assistant
      user:
        - user
    
    roles_to_train: ["assistant"]
    train_on_eos: "turn"

#we dont need this for Llama training because the chat template is llama3 and is consistent with the tokenization
special_tokens:
  pad_token: "<|end_of_text|>"
