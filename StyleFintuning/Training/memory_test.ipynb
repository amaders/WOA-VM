{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b270c200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/WOA-VM/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# alpha_probe.py  -- run in the same venv as Axolotl\n",
    "import os, gc, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "297d9b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:15<00:00,  3.87s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak @ s=1536: 31.84 GB, @ s=2048: 32.88 GB\n",
      "L=32, H=4096, B=4, dtype bytes=2\n",
      "alpha ≈ 2.089  (BF16, GC=on)\n"
     ]
    }
   ],
   "source": [
    "# Match your YAML\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "USE_BF16 = True\n",
    "USE_GC   = True   # gradient checkpointing\n",
    "B        = 4      # micro_batch_size\n",
    "S1, S2   = 1536, 2048   # two lengths inside your 2048 cap\n",
    "\n",
    "# Make sure we use flash-attn if installed (memory is then linear in s)\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype  = torch.bfloat16 if USE_BF16 else torch.float16\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=dtype,\n",
    "    device_map={\"\":0}  # single GPU\n",
    ").train()\n",
    "\n",
    "# Match Axolotl's training toggles\n",
    "if USE_GC:\n",
    "    model.gradient_checkpointing_enable()\n",
    "# ensure no KV cache is kept during training\n",
    "if hasattr(model.config, \"use_cache\"):\n",
    "    model.config.use_cache = False\n",
    "# prefer flash-attn v2 if available in this Transformers build\n",
    "try:\n",
    "    model.config.attn_implementation = \"flash_attention_2\"\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def peak_bytes_for(seq_len: int) -> int:\n",
    "    gc.collect(); torch.cuda.empty_cache(); torch.cuda.reset_peak_memory_stats()\n",
    "    # random tokens; labels=input_ids triggers full LM loss/backward like SFT\n",
    "    x = torch.randint(low=0, high=tok.vocab_size, size=(B, seq_len), device=device)\n",
    "    out = model(input_ids=x, attention_mask=torch.ones_like(x), labels=x)\n",
    "    loss = out.loss\n",
    "    loss.backward()\n",
    "    torch.cuda.synchronize()\n",
    "    peak = torch.cuda.max_memory_allocated()\n",
    "    # cleanup grads to avoid accumulation\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    return peak\n",
    "\n",
    "bytes1 = peak_bytes_for(S1)\n",
    "bytes2 = peak_bytes_for(S2)\n",
    "\n",
    "# Read actual model dims so we don't hardcode L,H\n",
    "cfg = model.config\n",
    "L = getattr(cfg, \"num_hidden_layers\", None)\n",
    "H = getattr(cfg, \"hidden_size\", None)\n",
    "assert L and H, \"Could not read num_hidden_layers/hidden_size\"\n",
    "\n",
    "bytes_per_elem = 2 if USE_BF16 else 2  # both bf16/fp16 are 2 bytes\n",
    "\n",
    "# α from slope:  peak ≈ const + (B * s * L * H * α * bytes_per_elem)\n",
    "slope_bytes_per_token = (bytes2 - bytes1) / (S2 - S1)\n",
    "alpha = slope_bytes_per_token / (B * L * H * bytes_per_elem)\n",
    "\n",
    "gb = 1024**3\n",
    "print(f\"Peak @ s={S1}: {bytes1/gb:.2f} GB, @ s={S2}: {bytes2/gb:.2f} GB\")\n",
    "print(f\"L={L}, H={H}, B={B}, dtype bytes={bytes_per_elem}\")\n",
    "print(f\"alpha ≈ {alpha:.3f}  (BF16, GC={'on' if USE_GC else 'off'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d144bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
